{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joshua Black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring 'nb1' subset with text analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import logging\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore, TfidfModel\n",
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_cytoscape as cyto\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "import NL_helpers\n",
    "import NL_topicmodels # Will need to generate BOW using function in topic models.\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "TOKENIZER = RegexpTokenizer(r\"[A-Za-z']+\")\n",
    "STOPWORDS = set(stopwords.words())\n",
    "WORDS = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to reload NL_helpers and NL_topicmodels if they have been changed.\n",
    "from importlib import reload\n",
    "reload(NL_helpers)\n",
    "reload(NL_topicmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already generated a dataset containing all articles with the regex search term 'philoso*' run on the NL library. I load it as follows:\n",
    "\n",
    "**Note, error:** the step at which I apply the search is before the text has been converted to lower case. The results of the re search are case sensitive. I had intended to include, e.g. 'Philosophy' as well. I don't expect that this will cause too many problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df = pd.read_pickle('pickles/nb1_philoso_df.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LT_18940102_ARTICLE9</th>\n",
       "      <td>HOW THEY GET ENGAGED IN GREENLAND.</td>\n",
       "      <td>[The missionaries in Greenland seem to have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT_18940102_ARTICLE12</th>\n",
       "      <td>SHEARING WET SHEEP.</td>\n",
       "      <td>[TO THE EDITOR. Sir, —It being a matter of gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT_18940102_ARTICLE13</th>\n",
       "      <td>WORK AND WAGES.</td>\n",
       "      <td>[TO THE EDITOR. Sir, —It appears that no cry o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT_18940102_ARTICLE15</th>\n",
       "      <td>ROMANTIC WOMEN.</td>\n",
       "      <td>[Most women are inclined to be romantic. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT_18940102_ARTICLE41</th>\n",
       "      <td>IN MEMORIAM.</td>\n",
       "      <td>[BISHOP HARPER. Ob., I)K. SS, 1533. “Go forth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODT_18790121_ARTICLE9</th>\n",
       "      <td>THE PRESBYTERIAN SYNOD OF OTAGO AND SOUTHLAND.</td>\n",
       "      <td>[FIFTH DAY. The Syrorl resumed its transaction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODT_18790121_ARTICLE15</th>\n",
       "      <td>THE SECOLD CHAIR OF MORAL PHILOSOPHY AND POLIT...</td>\n",
       "      <td>[TO TJIE EDITOR, Sir,—You must not suppose iro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODT_18790121_ARTICLE16</th>\n",
       "      <td>GODLESS EDUCATION. TO THE EDITOR</td>\n",
       "      <td>[Sir,—l clxervo that the reft-reuce to tbo \"jj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODT_18790121_ARTICLE17</th>\n",
       "      <td>THE BIBLE SCHOOLS. TO THE EDITOR.</td>\n",
       "      <td>[Sia,—lt may not be out of place, in the prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ODT_18790122_ARTICLE22</th>\n",
       "      <td>THE UNIVERSITY PROFESSORSHIP.</td>\n",
       "      <td>[TO THE EDITOR,, Sin,—rmust take exception to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287832 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Title  \\\n",
       "LT_18940102_ARTICLE9                   HOW THEY GET ENGAGED IN GREENLAND.   \n",
       "LT_18940102_ARTICLE12                                 SHEARING WET SHEEP.   \n",
       "LT_18940102_ARTICLE13                                     WORK AND WAGES.   \n",
       "LT_18940102_ARTICLE15                                     ROMANTIC WOMEN.   \n",
       "LT_18940102_ARTICLE41                                        IN MEMORIAM.   \n",
       "...                                                                   ...   \n",
       "ODT_18790121_ARTICLE9      THE PRESBYTERIAN SYNOD OF OTAGO AND SOUTHLAND.   \n",
       "ODT_18790121_ARTICLE15  THE SECOLD CHAIR OF MORAL PHILOSOPHY AND POLIT...   \n",
       "ODT_18790121_ARTICLE16                   GODLESS EDUCATION. TO THE EDITOR   \n",
       "ODT_18790121_ARTICLE17                  THE BIBLE SCHOOLS. TO THE EDITOR.   \n",
       "ODT_18790122_ARTICLE22                      THE UNIVERSITY PROFESSORSHIP.   \n",
       "\n",
       "                                                                     Text  \n",
       "LT_18940102_ARTICLE9    [The missionaries in Greenland seem to have a ...  \n",
       "LT_18940102_ARTICLE12   [TO THE EDITOR. Sir, —It being a matter of gre...  \n",
       "LT_18940102_ARTICLE13   [TO THE EDITOR. Sir, —It appears that no cry o...  \n",
       "LT_18940102_ARTICLE15   [Most women are inclined to be romantic. This ...  \n",
       "LT_18940102_ARTICLE41   [BISHOP HARPER. Ob., I)K. SS, 1533. “Go forth ...  \n",
       "...                                                                   ...  \n",
       "ODT_18790121_ARTICLE9   [FIFTH DAY. The Syrorl resumed its transaction...  \n",
       "ODT_18790121_ARTICLE15  [TO TJIE EDITOR, Sir,—You must not suppose iro...  \n",
       "ODT_18790121_ARTICLE16  [Sir,—l clxervo that the reft-reuce to tbo \"jj...  \n",
       "ODT_18790121_ARTICLE17  [Sia,—lt may not be out of place, in the prese...  \n",
       "ODT_18790122_ARTICLE22  [TO THE EDITOR,, Sin,—rmust take exception to ...  \n",
       "\n",
       "[287832 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "philoso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df['Tokenised'] = philoso_df['Text'].apply(\n",
    "    lambda x: TOKENIZER.tokenize(NL_helpers.blocks2string(x).lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates ought to be removed from 'subset' corpus, but have already been removed from full 'philoso*' corpus.\n",
    "# philoso_df = philoso_df[~philoso_df.astype(str).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LT         38237\n",
       "ODT        18353\n",
       "ESD        17870\n",
       "OW         15540\n",
       "AS         15469\n",
       "CHP        13972\n",
       "HBH        13734\n",
       "MEX        10722\n",
       "NZTIM      10025\n",
       "BH          8826\n",
       "GRA         7720\n",
       "DSC         7598\n",
       "TC          7230\n",
       "NOT         7035\n",
       "LWM         7012\n",
       "OAM         6506\n",
       "NEM         6174\n",
       "ME          5660\n",
       "WH          5481\n",
       "WC          4977\n",
       "WT          4966\n",
       "DTN         4717\n",
       "MIC         4487\n",
       "HNS         4062\n",
       "WCT         3865\n",
       "AG          3699\n",
       "WSTAR       3123\n",
       "WDT         3107\n",
       "WI          2903\n",
       "CROMARG     2799\n",
       "FS          2376\n",
       "MH          1976\n",
       "HBT         1844\n",
       "DUNST       1595\n",
       "WEST        1505\n",
       "WAIST       1483\n",
       "IT          1433\n",
       "HAST        1266\n",
       "BA          1120\n",
       "LCP         1047\n",
       "CL           777\n",
       "MS           736\n",
       "MT           723\n",
       "KUMAT        708\n",
       "WOODEX       692\n",
       "OO           629\n",
       "NA           482\n",
       "NZSCSG       387\n",
       "OG           300\n",
       "OPUNT        220\n",
       "HBWT         176\n",
       "HLC          172\n",
       "NZGWS        101\n",
       "WDA           78\n",
       "NZCPNA        58\n",
       "LTCBG         34\n",
       "MTBM          21\n",
       "ALG           10\n",
       "NZABIG         9\n",
       "CHARG          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts by newspaper\n",
    "philoso_df.index.map(lambda x: x[0:x.find('_')]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows that we have 29575 items containing 'philoso\\*' in the whole dataset. This is between two and three times the size of the 'starter corpus', but is significantly smaller than the total corpus. **TODO: put full size here.**\n",
    "\n",
    "I've also produced a count for each newspaper. OW - the Otago Witness tops the list. Surprising as this was a weekly paper and so should be expected to have less total issues than the others. However, it might have had more 'intellectual' content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3408062850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counts by year.\n",
    "philoso_df.index.map(lambda x: int(x[x.find('_')+1:x.find('_')+5])).to_series().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting a subset of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a random subset of the corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = sample(list(philoso_df.index), 100)\n",
    "interact(NL_helpers.html_text, index=sample_indices, dataframe=fixed(philoso_df), boldface=fixed('philoso*\\\\w*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by quickly looking through concordances for key words. The initial 'flat list' of tokens is taken from a random sample of 1000 items in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for tokens in philoso_df['Tokenised'].sample(n=1000, random_state=1):\n",
    "    for token in tokens:\n",
    "        flat_list.append(token)\n",
    "sample_text = Text(flat_list)\n",
    "del flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook keeps the full dataset in memory. I'll be using 'del' to remove references to large objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, 'philosophy':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text.concordance('philosophy', width=100, lines=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_text.concordance('lecture', width=100, lines=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations shows which words have a tendency to appear together *near one another* in documents (cf. co-occurrence). The ranking of word pairs depends on the statistic chosen.\n",
    "\n",
    "**PMI:** pointwise mutual information is a measure from information theory. Suppose we have probability distributions for each word, *x* and *y*, we might wonder how much information about p(x) is carried by p(y). If the words *x* and *y* always occur together, then *pmi* will be very high. This makes PMI a good measure of word occurence (dreadful - come back once you've reminded yourself how this works).\n",
    "\n",
    "**Likelihood ratio:**\n",
    "\n",
    "The code below collects all immediate bigrams for a window of size 2 and of size 5. We then filter our bigrams which do not contain 'philosophy', those which appear less than three times, and those which contain stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again build up a flat list, but this time with the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for tokens in philoso_df['Tokenised']:\n",
    "    for token in tokens:\n",
    "        flat_list.append(token)\n",
    "all_text = Text(flat_list)\n",
    "del flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_filter = lambda *w: 'philosophy' not in w\n",
    "stopword_filter = lambda w: w in STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf = BigramCollocationFinder.from_words(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf.apply_ngram_filter(philoso_filter)\n",
    "bcf.apply_word_filter(stopword_filter)\n",
    "bcf.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_ws2 = bcf.nbest(bm.pmi, 50)\n",
    "pmi_ws2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a window size of two, we get words either side of 'philosophy'.\n",
    "Notes:\n",
    " - natural comes up again, along with 'experimental', 'positive', 'inductive'. \n",
    " - 'pig philosophy' is, I think, a term of Carylye's - suggest some specific discussions.\n",
    " - most suggest intellectual discussion (except maybe 'proverbial'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ws2 = bcf.nbest(bm.likelihood_ratio, 50)\n",
    "lr_ws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_not_lr = []\n",
    "for collocation in pmi_ws2:\n",
    "    if collocation not in lr_ws2:\n",
    "        pmi_not_lr.append(collocation)\n",
    "pmi_not_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_not_pmi = []\n",
    "for collocation in lr_ws2:\n",
    "    if collocation not in pmi_ws2:\n",
    "        lr_not_pmi.append(collocation)\n",
    "lr_not_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences here don't seem to make a big difference to the conclusions above. Not surprising as they are very closely related measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bcf\n",
    "del all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also expand the window to consider words either five spaces to the left or right of philosophy. (Window size includes stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_big = BigramCollocationFinder.from_words(all_text, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_big.apply_ngram_filter(philoso_filter)\n",
    "bcf_big.apply_word_filter(stopword_filter)\n",
    "bcf_big.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_ws5 = bcf_big.nbest(bm.pmi, 50)\n",
    "pmi_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_ws5 = bcf_big.nbest(bm.likelihood_ratio, 50)\n",
    "lr_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_not_lr_ws5 = []\n",
    "for collocation in pmi_ws5:\n",
    "    if collocation not in lr_ws5:\n",
    "        pmi_not_lr_ws5.append(collocation)\n",
    "pmi_not_lr_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_not_pmi_ws5 = []\n",
    "for collocation in lr_ws5:\n",
    "    if collocation not in pmi_ws5:\n",
    "        lr_not_pmi_ws5.append(collocation)\n",
    "lr_not_pmi_ws5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More difference in results here (more ngrams to choose between). But neither list gives a particularly different impression of the corpus as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr_not_pmi_ws5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bcf_big, all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences in scoring more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform document-level cooccurence analysis we will shift from NLTK to gensim. We first build a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary is much too large for our purposes (at ~70k words). We can filter stop words and all one and two letter words from the tokenized text to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df['Tokenised'] = philoso_df['Tokenised'].map(lambda x: [word for word in x if len(word)>2 and not word in STOPWORDS and word in WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me here\n",
    "philoso_df['Entities'] = philoso_df['Entities'].map(lambda x: [i.lower() for i in x if len(i.lower()) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-19 09:37:35,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-19 09:37:38,023 : INFO : adding document #10000 to Dictionary(28283 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:40,476 : INFO : adding document #20000 to Dictionary(34496 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:42,809 : INFO : adding document #30000 to Dictionary(37969 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:45,073 : INFO : adding document #40000 to Dictionary(39673 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:47,406 : INFO : adding document #50000 to Dictionary(41834 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:49,311 : INFO : adding document #60000 to Dictionary(43345 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:51,465 : INFO : adding document #70000 to Dictionary(44813 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:53,646 : INFO : adding document #80000 to Dictionary(45903 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:56,111 : INFO : adding document #90000 to Dictionary(47737 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:37:58,512 : INFO : adding document #100000 to Dictionary(48930 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:00,504 : INFO : adding document #110000 to Dictionary(49868 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:02,967 : INFO : adding document #120000 to Dictionary(50903 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:05,734 : INFO : adding document #130000 to Dictionary(51980 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:07,916 : INFO : adding document #140000 to Dictionary(52746 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:09,956 : INFO : adding document #150000 to Dictionary(53238 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:12,288 : INFO : adding document #160000 to Dictionary(53783 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:14,530 : INFO : adding document #170000 to Dictionary(54168 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:17,249 : INFO : adding document #180000 to Dictionary(54800 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:19,941 : INFO : adding document #190000 to Dictionary(55513 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:22,055 : INFO : adding document #200000 to Dictionary(56058 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:24,470 : INFO : adding document #210000 to Dictionary(56382 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:26,649 : INFO : adding document #220000 to Dictionary(56802 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:28,799 : INFO : adding document #230000 to Dictionary(57207 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:30,948 : INFO : adding document #240000 to Dictionary(57571 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:33,771 : INFO : adding document #250000 to Dictionary(58429 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:36,534 : INFO : adding document #260000 to Dictionary(58741 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:38,430 : INFO : adding document #270000 to Dictionary(58799 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:40,347 : INFO : adding document #280000 to Dictionary(59048 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...)\n",
      "2021-01-19 09:38:42,505 : INFO : built Dictionary(59457 unique tokens: ['accept', 'another', 'away', 'believe', 'bless']...) from 287832 documents (total 59671205 corpus positions)\n",
      "2021-01-19 09:38:42,599 : INFO : discarding 33155 tokens: [('another', 66342), ('away', 42107), ('believe', 53966), ('day', 92052), ('find', 64249), ('god', 37544), ('good', 113636), ('home', 39597), ('ing', 89779), ('last', 84907)]...\n",
      "2021-01-19 09:38:42,600 : INFO : keeping 26302 tokens which were in no less than 25 and no more than 28783 (=10.0%) documents\n",
      "2021-01-19 09:38:42,638 : INFO : resulting dictionary: Dictionary(26302 unique tokens: ['accept', 'bless', 'blushing', 'busy', 'ceremony']...)\n"
     ]
    }
   ],
   "source": [
    "minimum_in_docs = 25 # 10\n",
    "max_prop = 0.1\n",
    "dictionary = corpora.Dictionary(philoso_df['Tokenised'])\n",
    "dictionary.filter_extremes(no_below=minimum_in_docs, no_above=max_prop)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-19 09:38:42,660 : INFO : saving Dictionary object under dictionaries/nb1_df_min25_max10perc.dict, separately None\n",
      "2021-01-19 09:38:42,676 : INFO : saved dictionaries/nb1_df_min25_max10perc.dict\n"
     ]
    }
   ],
   "source": [
    "dictionary.save(f'dictionaries/nb1_df_min{minimum_in_docs}_max{int(max_prop*100)}perc.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df['BOW'] = philoso_df['Entities'].apply(lambda x: dictionary.doc2bow(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus = NL_topicmodels.NL_corpus(philoso_df, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(philo_corpus, dictionary=philo_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items['TF-IDF'] = tfidf_model[philo_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes over time? - I don't think this is helping me. Need some more non-philosophy as a background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = corpus2csc(philo_corpus.items['BOW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame.sparse.from_spmatrix(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.index = dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.to_pickle('pickles/nb1_dtm_26kfiltered_10pc_dict.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = dtm.astype('Sparse[int8]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.clip(lower=0,upper=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df = binary_dtm.dot(binary_dtm.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = sparse.dot(sparse.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.sparse.save_npz('pickles/tt_nb1_sparse.npz', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df = pd.DataFrame.sparse.from_spmatrix(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df.index = dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df.columns = dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df.to_pickle('pickles/nb1_tt_26kfiltered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html\n",
    "def dice_coocs(term, dtm, ttm, num_coocs):\n",
    "    \"\"\"Return num_coocs with dice statistics given search term\n",
    "    document-term matrix and term-term matrix. Return as \n",
    "    pandas series with terms as indices and significances as values..\n",
    "    ttm and dtm are pandas dataframes.\"\"\"\n",
    "    #num_documents = len(dtm.columns)\n",
    "    all_term_occurrences = dtm.sum(axis=1)\n",
    "    term_occurrences = all_term_occurrences[term]\n",
    "    cooccurrences = ttm.loc[term]\n",
    "    dicesig = 2 * cooccurrences / (term_occurrences + all_term_occurrences)\n",
    "    dicesig = dicesig.sort_values(ascending=False)[0:num_coocs]\n",
    "    return dicesig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicesig = 2 * cooccurrences / (term_occurrences + all_term_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dicesig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicesig.sort_values(ascending=False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coocs('philosophy', binary_dtm, tt_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_helpers.log_dice_coocs('infinite', dtm, tt_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_cooc = dice_coocs('philosophy', binary_dtm, tt_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_network = NL_helpers.network_dataframe(\n",
    "    term='philosophy', \n",
    "    stat='ml', \n",
    "    dtm=dtm,\n",
    "    ttm=tt_df,\n",
    "    num_coocs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_network_df = pd.DataFrame(data=philo_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "for item in philo_cooc.iteritems():\n",
    "    from_list = network.get('source', [])\n",
    "    from_list.append('philosophy')\n",
    "    network['source'] = from_list\n",
    "    to_list = network.get('target', [])\n",
    "    to_list.append(item[0])\n",
    "    network['target'] = to_list\n",
    "    weight_list = network.get('weight', [])\n",
    "    weight_list.append(item[1])\n",
    "    network['weight'] = weight_list\n",
    "    \n",
    "    item_coocs = dice_coocs(item[0], binary_dtm, tt_df, 10)\n",
    "    for sub_item in item_coocs.iteritems():\n",
    "        if item[0] != sub_item[0]:\n",
    "            from_list = network.get('source', [])\n",
    "            from_list.append(item[0])\n",
    "            network['source'] = from_list\n",
    "            to_list = network.get('target', [])\n",
    "            to_list.append(sub_item[0])\n",
    "            network['target'] = to_list\n",
    "            weight_list = network.get('weight', [])\n",
    "            weight_list.append(sub_item[1])\n",
    "            network['weight'] = weight_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.DataFrame(data=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.convert_matrix.from_pandas_edgelist(philo_network_df, edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = G.edges()\n",
    "weights = [np.exp(G[u][v]['weight']) * 0.01 for u,v in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nx.degree(G)\n",
    "sizes = [(d[node]+1) * 100 for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.drawing.layout.spring_layout(G, k=1/np.sqrt(len(G.nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "# Visualize graph components\n",
    "nx.draw_networkx_edges(G, layout, alpha=0.5, width=weights, edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G, layout, node_size=sizes, node_color=\"#210070\", alpha=0.9)\n",
    "label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}\n",
    "labels = nx.draw_networkx_labels(G, layout, font_size=11, bbox=label_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_net = NL_helpers.network_dash(\n",
    "    term='stout', \n",
    "    stat='log dice', \n",
    "    dtm=dtm,\n",
    "    ttm=tt_df,\n",
    "    num_coocs=25,\n",
    "    sec_coocs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)\n",
    "\n",
    "philo_cytoscape = cyto.Cytoscape(\n",
    "        id='philosophy-network',\n",
    "        minZoom=1,\n",
    "        layout={'name': 'cose'},\n",
    "        style={'width': '100%', 'height': '800px'},\n",
    "        elements=philo_net,\n",
    "        stylesheet=[\n",
    "            {\n",
    "                'selector': 'edge',\n",
    "                'style': {\n",
    "                    'width': 'mapData(weight, 3, 6, 1, 3)',\n",
    "                    'line-color': 'silver'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'selector': 'node',\n",
    "                'style': {\n",
    "                    'content': 'data(label)',\n",
    "                    'width': 'mapData(size, 1, 10, 10, 20)',\n",
    "                    'height': 'mapData(size, 1, 10, 10, 20)'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'selector': 'label',\n",
    "                'style': {\n",
    "                    'font-size': 6,\n",
    "                    'text-valign': 'center',\n",
    "                    'text-background-color': 'white',\n",
    "                    'text-background-opacity': 0.6,\n",
    "                    'text-background-padding': 1,\n",
    "                    'text-border-color': 'black',\n",
    "                    'text-border-opacity': 1,\n",
    "                    'text-border-width': 0.5\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H2(\"Cooccurence Networks (BOW)\"),\n",
    "    html.P(\"Search Term:\"),\n",
    "    dcc.Input(\n",
    "        id='search-term',\n",
    "        type='text',\n",
    "        value='stout'\n",
    "    ),\n",
    "    html.P(\"Statistic:\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stat-choice',\n",
    "        options=[\n",
    "            {'label': 'Mutual likelihood', 'value': 'ml'},\n",
    "            {'label': 'Log Dice', 'value': 'log dice'}\n",
    "        ],\n",
    "        value='ml'\n",
    "    ),\n",
    "    html.Button('Submit', id='submit-val', n_clicks=0),\n",
    "    philo_cytoscape\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='philosophy-network', component_property='elements'),\n",
    "    Input(component_id='submit-val', component_property='n_clicks'),\n",
    "    State(component_id='stat-choice', component_property='value'),\n",
    "    State(component_id='search-term', component_property='value'),\n",
    ")\n",
    "def update_network_stat(n_clicks, stat_value, search_value):\n",
    "    network = NL_helpers.network_dash(\n",
    "        term=search_value, \n",
    "        stat=stat_value, \n",
    "        dtm=dtm,\n",
    "        ttm=tt_df,\n",
    "        num_coocs=10,\n",
    "        sec_coocs=5\n",
    "    )\n",
    "    return network \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, mode='inline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
