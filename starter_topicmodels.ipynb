{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA601\n",
        "Joshua Black\n",
        "## Starter Kit Experiments in Processing and Initial Filtering.\n",
        "This notebook presents initial attempts at processing the National Library\n",
        "newspaper data, the result of using topic models to find genres\n",
        "of writing interesting for investigating philosophical writing within the\n",
        "'starter kit' corpus and for finding the topics covered within those genres.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Starter Pack\n",
        "The Starter Pack is around 2GB uncompressed and contains articles from\n",
        "- Charleston argus.\n",
        "- Hot lakes chronicle.\n",
        "- Lyell times and Central Buller gazette.\n",
        "- Mt. Benger mail.\n",
        "- The New Zealand gazette and Wellington spectator.\n",
        "- The Oxford observer : and Canterbury democrat.\n",
        "- Victoria times.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Processing\n",
        "The Starter Pack is given uncompressed. This is not true of the full dataset\n",
        "and will require a slightly different method.\n",
        "\n",
        "We begin by importing my helper functions and reading through the Starter Pack\n",
        "directories to find top-level folders for each issue in the Starter Pack.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# Remove before exporting notebook\n",
        "sys.path.append('/home/joshua/hdd/Documents/MADS/DATA601/')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from NL_helpers import *\n",
        "from NL_topicmodels import *\n",
        "\n",
        "PATH = \"/home/joshua/hdd/Documents/MADS/DATA601/NPOD_Starter/\"\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_walk = os.walk(PATH)\n",
        "\n",
        "# Collect issue folders using regex. All are of form NEWSPAPERCODE_DATE,\n",
        "# where date is in format YYYYMMDD\n",
        "issue_directories = {}\n",
        "for location in path_walk:\n",
        "    match = re.search(\"[A-Z]*_\\d{8}$\", location[0])\n",
        "    if match:\n",
        "        issue_directories[match.group(0)] = location[0] + '/'\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having collected the directories for each issue, we can collect the\n",
        "information we want from each. In this case, we parse the XML to produce\n",
        "a Python dictionary with an article id as key, and the newspaper, date,\n",
        "title, text, and tokenised text as values.\n",
        "\n",
        "The raw text is given as a list of strings, where each string corresponds to\n",
        "a 'text block' in the original newspaper scans. The tokenised text\n",
        "is tokenised the python NLTK regex tokeniser and default NLTK list of\n",
        "stopwords.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_dict = {}\n",
        "for issue, directory in issue_directories.items():\n",
        "    newspaper = issue[:-9]\n",
        "    date = issue[-8:]\n",
        "    articles = issue2articles(directory)\n",
        "    for article_code, title_and_text in articles.items():\n",
        "        article_code = article_code[7:] # remove 'MODSMD_' from article code\n",
        "        item_id = '_'.join([issue, article_code])\n",
        "        title, text = title_and_text\n",
        "        tokenised_and_stopped = tokenise_and_stop(text)\n",
        "        corpus_dict[item_id] = (\n",
        "            newspaper,\n",
        "            date,\n",
        "            title,\n",
        "            text,\n",
        "            tokenised_and_stopped\n",
        "        )\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now convert this dictionary to a pandas dataframe. We use the object datatype\n",
        "in order store Python lists within it. We save it as a pickle, also to enable\n",
        "storage which respects Python datatypes.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_df = pd.DataFrame.from_dict(\n",
        "    corpus_dict,\n",
        "    orient='index',\n",
        "    dtype = object,\n",
        "    columns=['Newspaper', 'Date', 'Title', 'Text', 'Tokenised']\n",
        "    )\n",
        "\n",
        "pickle_dir = '/home/joshua/hdd/Documents/MADS/DATA601/pickles/'\n",
        "corpus_df.to_pickle(pickle_dir + 'Starter_Items.tar.gz')\n",
        "corpus_df\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Topic Model Using Gensim\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/home/joshua/anaconda3/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}