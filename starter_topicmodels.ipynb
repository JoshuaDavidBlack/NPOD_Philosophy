{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# DATA601\n",
    "Joshua Black\n",
    "## Starter Kit Experiments in Processing and Initial Filtering.\n",
    "This notebook presents initial attempts at processing the National Library\n",
    "newspaper data, the result of using topic models to find genres\n",
    "of writing interesting for investigating philosophical writing within the\n",
    "'starter kit' corpus and for finding the topics covered within those genres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## The Starter Pack\n",
    "The Starter Pack is around 2GB uncompressed and contains articles from\n",
    "- Charleston argus.\n",
    "- Hot lakes chronicle.\n",
    "- Lyell times and Central Buller gazette.\n",
    "- Mt. Benger mail.\n",
    "- The New Zealand gazette and Wellington spectator.\n",
    "- The Oxford observer : and Canterbury democrat.\n",
    "- Victoria times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Initial Processing\n",
    "The Starter Pack is given uncompressed. This is not true of the full dataset\n",
    "and will require a slightly different method.\n",
    "\n",
    "We begin by importing my helper functions and reading through the Starter Pack\n",
    "directories to find top-level folders for each issue in the Starter Pack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Remove before exporting notebook\n",
    "sys.path.append('/home/joshua/hdd/Documents/MADS/DATA601/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import NL_helpers\n",
    "import NL_topicmodels\n",
    "\n",
    "PATH = \"/home/joshua/hdd/Documents/MADS/DATA601/NPOD_Starter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "path_walk = os.walk(PATH)\n",
    "\n",
    "# Collect issue folders using regex. All are of form NEWSPAPERCODE_DATE,\n",
    "# where date is in format YYYYMMDD\n",
    "issue_directories = {}\n",
    "for location in path_walk:\n",
    "    match = re.search(\"[A-Z]*_\\d{8}$\", location[0])\n",
    "    if match:\n",
    "        issue_directories[match.group(0)] = location[0] + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Having collected the directories for each issue, we can collect the\n",
    "information we want from each. In this case, we parse the XML to produce\n",
    "a Python dictionary with an article id as key, and the newspaper, date,\n",
    "title, text, and tokenised text as values.\n",
    "\n",
    "The raw text is given as a list of strings, where each string corresponds to\n",
    "a 'text block' in the original newspaper scans. The tokenised text\n",
    "is tokenised the python NLTK regex tokeniser and default NLTK list of\n",
    "stopwords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus_dict = {}\n",
    "for issue, directory in issue_directories.items():\n",
    "    newspaper = issue[:-9]\n",
    "    date = issue[-8:]\n",
    "    articles = NL_helpers.issue2articles(directory)\n",
    "    for article_code, title_and_text in articles.items():\n",
    "        article_code = article_code[7:] # remove 'MODSMD_' from article code\n",
    "        item_id = '_'.join([issue, article_code])\n",
    "        title, text = title_and_text\n",
    "        tokenised_and_stopped = NL_helpers.tokenise_and_stop(text)\n",
    "        corpus_dict[item_id] = (\n",
    "            newspaper,\n",
    "            date,\n",
    "            title,\n",
    "            text,\n",
    "            tokenised_and_stopped\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We now convert this dictionary to a pandas dataframe. We use the object datatype\n",
    "in order store Python lists within it. We save it as a pickle, also to enable\n",
    "storage which respects Python datatypes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokenised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHARG_18670302_ARTICLE1</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670302</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[ago a till last Thurs- with 40oz. of gold and...</td>\n",
       "      <td>[ago, last, thurs, 40oz, gold, went, back, sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE1</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>[1.33 2.25 3.15 2.â€”Halcyon, s.s., Wing, master...</td>\n",
       "      <td>[halcyon, wing, master, jane, schooner, julia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE2</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>CHARLESTON ARGUS.</td>\n",
       "      <td>[If the Pakihi district was but well supplied ...</td>\n",
       "      <td>[pakihi, district, well, supplied, water, nut,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE3</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[Some little excitement has been evinced in re...</td>\n",
       "      <td>[little, excitement, evinced, reference, tramw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE4</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>SATURDAY, MARCH 9, 1867. UNKNOWN</td>\n",
       "      <td>[3/u^ (Before C. Broad, Larceny. Ann Connelly,...</td>\n",
       "      <td>[broad, larceny, ann, connelly, woman, charged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE1</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>Wellington Tavern.</td>\n",
       "      <td>[Edward Davis begs to inform his friends and t...</td>\n",
       "      <td>[edward, davis, begs, inform, friends, public,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE2</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[Messrs Pratt and Bevan beg respectfully to in...</td>\n",
       "      <td>[messrs, pratt, bevan, beg, respectfully, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE3</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[We insert the following communication by, par...</td>\n",
       "      <td>[insert, following, communication, partic, ula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE4</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[The following Particulars were composed a sel...</td>\n",
       "      <td>[following, particulars, composed, selected, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE5</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>PLAN of THE CITY OF WELLINGTON Port Nicholson ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11516 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Newspaper      Date  \\\n",
       "CHARG_18670302_ARTICLE1     CHARG  18670302   \n",
       "CHARG_18670309_ARTICLE1     CHARG  18670309   \n",
       "CHARG_18670309_ARTICLE2     CHARG  18670309   \n",
       "CHARG_18670309_ARTICLE3     CHARG  18670309   \n",
       "CHARG_18670309_ARTICLE4     CHARG  18670309   \n",
       "...                           ...       ...   \n",
       "VT_18410915_ARTICLE1           VT  18410915   \n",
       "VT_18410915_ARTICLE2           VT  18410915   \n",
       "VT_18410915_ARTICLE3           VT  18410915   \n",
       "VT_18410915_ARTICLE4           VT  18410915   \n",
       "VT_18410915_ARTICLE5           VT  18410915   \n",
       "\n",
       "                                                                     Title  \\\n",
       "CHARG_18670302_ARTICLE1                                           UNTITLED   \n",
       "CHARG_18670309_ARTICLE1                                            UNKNOWN   \n",
       "CHARG_18670309_ARTICLE2                                  CHARLESTON ARGUS.   \n",
       "CHARG_18670309_ARTICLE3                                           UNTITLED   \n",
       "CHARG_18670309_ARTICLE4                   SATURDAY, MARCH 9, 1867. UNKNOWN   \n",
       "...                                                                    ...   \n",
       "VT_18410915_ARTICLE1                                    Wellington Tavern.   \n",
       "VT_18410915_ARTICLE2                                              UNTITLED   \n",
       "VT_18410915_ARTICLE3                                              UNTITLED   \n",
       "VT_18410915_ARTICLE4                                              UNTITLED   \n",
       "VT_18410915_ARTICLE5     PLAN of THE CITY OF WELLINGTON Port Nicholson ...   \n",
       "\n",
       "                                                                      Text  \\\n",
       "CHARG_18670302_ARTICLE1  [ago a till last Thurs- with 40oz. of gold and...   \n",
       "CHARG_18670309_ARTICLE1  [1.33 2.25 3.15 2.â€”Halcyon, s.s., Wing, master...   \n",
       "CHARG_18670309_ARTICLE2  [If the Pakihi district was but well supplied ...   \n",
       "CHARG_18670309_ARTICLE3  [Some little excitement has been evinced in re...   \n",
       "CHARG_18670309_ARTICLE4  [3/u^ (Before C. Broad, Larceny. Ann Connelly,...   \n",
       "...                                                                    ...   \n",
       "VT_18410915_ARTICLE1     [Edward Davis begs to inform his friends and t...   \n",
       "VT_18410915_ARTICLE2     [Messrs Pratt and Bevan beg respectfully to in...   \n",
       "VT_18410915_ARTICLE3     [We insert the following communication by, par...   \n",
       "VT_18410915_ARTICLE4     [The following Particulars were composed a sel...   \n",
       "VT_18410915_ARTICLE5                                                    []   \n",
       "\n",
       "                                                                 Tokenised  \n",
       "CHARG_18670302_ARTICLE1  [ago, last, thurs, 40oz, gold, went, back, sma...  \n",
       "CHARG_18670309_ARTICLE1  [halcyon, wing, master, jane, schooner, julia,...  \n",
       "CHARG_18670309_ARTICLE2  [pakihi, district, well, supplied, water, nut,...  \n",
       "CHARG_18670309_ARTICLE3  [little, excitement, evinced, reference, tramw...  \n",
       "CHARG_18670309_ARTICLE4  [broad, larceny, ann, connelly, woman, charged...  \n",
       "...                                                                    ...  \n",
       "VT_18410915_ARTICLE1     [edward, davis, begs, inform, friends, public,...  \n",
       "VT_18410915_ARTICLE2     [messrs, pratt, bevan, beg, respectfully, info...  \n",
       "VT_18410915_ARTICLE3     [insert, following, communication, partic, ula...  \n",
       "VT_18410915_ARTICLE4     [following, particulars, composed, selected, m...  \n",
       "VT_18410915_ARTICLE5                                                    []  \n",
       "\n",
       "[11516 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame.from_dict(\n",
    "    corpus_dict,\n",
    "    orient='index',\n",
    "    dtype = object,\n",
    "    columns=['Newspaper', 'Date', 'Title', 'Text', 'Tokenised']\n",
    "    )\n",
    "\n",
    "pickle_dir = '/home/joshua/hdd/Documents/MADS/DATA601/pickles/'\n",
    "corpus_df.to_pickle(pickle_dir + 'Starter_Items.tar.gz')\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Initial Topic Model Using Gensim\n",
    "Earlier experiments have shown that having a bunch of empty documents around\n",
    "is not good for producing interesting models. I found an interesting looking\n",
    "topic filled with words in te reo, but was disappointed to find it represented\n",
    "a very small number of actual documents and a huge number of empty ones.\n",
    "I will use two filtering steps. First, I filter out those articles which\n",
    "have less than 20 words after tokenising.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "cutoff = 20\n",
    "filtered_corpus_df = corpus_df[corpus_df['Tokenised'].apply(lambda x: len(x) >= cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We then create a dictionary for applying topic models with Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_in_docs = 5\n",
    "dictionary = corpora.Dictionary(filtered_corpus_df['Tokenised'])\n",
    "dictionary.filter_extremes(no_below=minimum_in_docs, no_above=0.5)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('dictionaries/starter_pack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the pre-generated dictionary rather than generating it.\n",
    "dictionary = corpora.Dictionary.load('dictionaries/starter_pack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough to use the `NL_corpus` class provided in `NL_topicmodels.py`. This generates a bag of words representation of each article on initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokenised</th>\n",
       "      <th>BOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHARG_18670302_ARTICLE1</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670302</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[ago a till last Thurs- with 40oz. of gold and...</td>\n",
       "      <td>[ago, last, thurs, 40oz, gold, went, back, sma...</td>\n",
       "      <td>[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE1</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>[1.33 2.25 3.15 2.â€”Halcyon, s.s., Wing, master...</td>\n",
       "      <td>[halcyon, wing, master, jane, schooner, julia,...</td>\n",
       "      <td>[(27, 1), (49, 1), (167, 1), (244, 1), (342, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE2</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>CHARLESTON ARGUS.</td>\n",
       "      <td>[If the Pakihi district was but well supplied ...</td>\n",
       "      <td>[pakihi, district, well, supplied, water, nut,...</td>\n",
       "      <td>[(10, 1), (11, 1), (29, 1), (37, 1), (48, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE3</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[Some little excitement has been evinced in re...</td>\n",
       "      <td>[little, excitement, evinced, reference, tramw...</td>\n",
       "      <td>[(8, 1), (13, 2), (15, 1), (47, 1), (48, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARG_18670309_ARTICLE4</th>\n",
       "      <td>CHARG</td>\n",
       "      <td>18670309</td>\n",
       "      <td>SATURDAY, MARCH 9, 1867. UNKNOWN</td>\n",
       "      <td>[3/u^ (Before C. Broad, Larceny. Ann Connelly,...</td>\n",
       "      <td>[broad, larceny, ann, connelly, woman, charged...</td>\n",
       "      <td>[(132, 1), (170, 1), (230, 1), (244, 1), (371,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OO_18990909_ARTICLE11</th>\n",
       "      <td>OO</td>\n",
       "      <td>18990909</td>\n",
       "      <td>OXFOED ROAD BOARD.</td>\n",
       "      <td>[The ordinary meeting of the Board was held on...</td>\n",
       "      <td>[ordinary, meeting, board, held, welnesday, se...</td>\n",
       "      <td>[(6, 3), (71, 1), (120, 1), (125, 2), (139, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE1</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>Wellington Tavern.</td>\n",
       "      <td>[Edward Davis begs to inform his friends and t...</td>\n",
       "      <td>[edward, davis, begs, inform, friends, public,...</td>\n",
       "      <td>[(120, 1), (135, 1), (139, 1), (143, 1), (147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE2</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[Messrs Pratt and Bevan beg respectfully to in...</td>\n",
       "      <td>[messrs, pratt, bevan, beg, respectfully, info...</td>\n",
       "      <td>[(86, 1), (105, 1), (122, 1), (147, 1), (170, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE3</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[We insert the following communication by, par...</td>\n",
       "      <td>[insert, following, communication, partic, ula...</td>\n",
       "      <td>[(9, 1), (11, 1), (48, 2), (54, 1), (97, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT_18410915_ARTICLE4</th>\n",
       "      <td>VT</td>\n",
       "      <td>18410915</td>\n",
       "      <td>UNTITLED</td>\n",
       "      <td>[The following Particulars were composed a sel...</td>\n",
       "      <td>[following, particulars, composed, selected, m...</td>\n",
       "      <td>[(13, 1), (16, 1), (37, 1), (51, 2), (60, 1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10460 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Newspaper      Date                             Title  \\\n",
       "CHARG_18670302_ARTICLE1     CHARG  18670302                          UNTITLED   \n",
       "CHARG_18670309_ARTICLE1     CHARG  18670309                           UNKNOWN   \n",
       "CHARG_18670309_ARTICLE2     CHARG  18670309                 CHARLESTON ARGUS.   \n",
       "CHARG_18670309_ARTICLE3     CHARG  18670309                          UNTITLED   \n",
       "CHARG_18670309_ARTICLE4     CHARG  18670309  SATURDAY, MARCH 9, 1867. UNKNOWN   \n",
       "...                           ...       ...                               ...   \n",
       "OO_18990909_ARTICLE11          OO  18990909                OXFOED ROAD BOARD.   \n",
       "VT_18410915_ARTICLE1           VT  18410915                Wellington Tavern.   \n",
       "VT_18410915_ARTICLE2           VT  18410915                          UNTITLED   \n",
       "VT_18410915_ARTICLE3           VT  18410915                          UNTITLED   \n",
       "VT_18410915_ARTICLE4           VT  18410915                          UNTITLED   \n",
       "\n",
       "                                                                      Text  \\\n",
       "CHARG_18670302_ARTICLE1  [ago a till last Thurs- with 40oz. of gold and...   \n",
       "CHARG_18670309_ARTICLE1  [1.33 2.25 3.15 2.â€”Halcyon, s.s., Wing, master...   \n",
       "CHARG_18670309_ARTICLE2  [If the Pakihi district was but well supplied ...   \n",
       "CHARG_18670309_ARTICLE3  [Some little excitement has been evinced in re...   \n",
       "CHARG_18670309_ARTICLE4  [3/u^ (Before C. Broad, Larceny. Ann Connelly,...   \n",
       "...                                                                    ...   \n",
       "OO_18990909_ARTICLE11    [The ordinary meeting of the Board was held on...   \n",
       "VT_18410915_ARTICLE1     [Edward Davis begs to inform his friends and t...   \n",
       "VT_18410915_ARTICLE2     [Messrs Pratt and Bevan beg respectfully to in...   \n",
       "VT_18410915_ARTICLE3     [We insert the following communication by, par...   \n",
       "VT_18410915_ARTICLE4     [The following Particulars were composed a sel...   \n",
       "\n",
       "                                                                 Tokenised  \\\n",
       "CHARG_18670302_ARTICLE1  [ago, last, thurs, 40oz, gold, went, back, sma...   \n",
       "CHARG_18670309_ARTICLE1  [halcyon, wing, master, jane, schooner, julia,...   \n",
       "CHARG_18670309_ARTICLE2  [pakihi, district, well, supplied, water, nut,...   \n",
       "CHARG_18670309_ARTICLE3  [little, excitement, evinced, reference, tramw...   \n",
       "CHARG_18670309_ARTICLE4  [broad, larceny, ann, connelly, woman, charged...   \n",
       "...                                                                    ...   \n",
       "OO_18990909_ARTICLE11    [ordinary, meeting, board, held, welnesday, se...   \n",
       "VT_18410915_ARTICLE1     [edward, davis, begs, inform, friends, public,...   \n",
       "VT_18410915_ARTICLE2     [messrs, pratt, bevan, beg, respectfully, info...   \n",
       "VT_18410915_ARTICLE3     [insert, following, communication, partic, ula...   \n",
       "VT_18410915_ARTICLE4     [following, particulars, composed, selected, m...   \n",
       "\n",
       "                                                                       BOW  \n",
       "CHARG_18670302_ARTICLE1  [(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...  \n",
       "CHARG_18670309_ARTICLE1  [(27, 1), (49, 1), (167, 1), (244, 1), (342, 2...  \n",
       "CHARG_18670309_ARTICLE2  [(10, 1), (11, 1), (29, 1), (37, 1), (48, 1), ...  \n",
       "CHARG_18670309_ARTICLE3  [(8, 1), (13, 2), (15, 1), (47, 1), (48, 2), (...  \n",
       "CHARG_18670309_ARTICLE4  [(132, 1), (170, 1), (230, 1), (244, 1), (371,...  \n",
       "...                                                                    ...  \n",
       "OO_18990909_ARTICLE11    [(6, 3), (71, 1), (120, 1), (125, 2), (139, 1)...  \n",
       "VT_18410915_ARTICLE1     [(120, 1), (135, 1), (139, 1), (143, 1), (147,...  \n",
       "VT_18410915_ARTICLE2     [(86, 1), (105, 1), (122, 1), (147, 1), (170, ...  \n",
       "VT_18410915_ARTICLE3     [(9, 1), (11, 1), (48, 2), (54, 1), (97, 1), (...  \n",
       "VT_18410915_ARTICLE4     [(13, 1), (16, 1), (37, 1), (51, 2), (60, 1), ...  \n",
       "\n",
       "[10460 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starter_corpus = NL_topicmodels.NL_corpus(filtered_corpus_df, dictionary)\n",
    "starter_corpus.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We filter again to remove any article whose BOW representation is less than 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10183"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starter_corpus.items = starter_corpus.items[starter_corpus.items['BOW'].apply(lambda x: len(x) >= cutoff)]\n",
    "len(starter_corpus.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that we have 10183 remaining documents.\n",
    "\n",
    "We run a topic model on these using `LdaMulticore` from `gensim`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_model = LdaMulticore(\n",
    "    starter_corpus,\n",
    "    num_topics= 50,\n",
    "    workers = 15,\n",
    "    chunksize = 220,\n",
    "    id2word=starter_corpus.dictionary,\n",
    "    iterations = 500,\n",
    "    passes = 25,\n",
    "    eval_every = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be visualised using `pyLDAvis` as follows: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NL_corpus' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4740dc1cd6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarter_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarter_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstarter_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0mterm_freqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Term frequencies and dictionary have different shape {} != {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_freqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Document lengths and corpus have different sizes {} != {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lda_alpha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NL_corpus' has no len()"
     ]
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim.prepare(starter_model, starter_corpus, dictionary=starter_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_kws = NL_topicmodels.topics_and_keywords(starter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0.022*\"race\" + 0.016*\"club\" + 0.015*\"match\" + 0.012*\"second\" + 0.012*\"first\" + 0.012*\"time\" + 0.010*\"mile\" + 0.010*\"oxford\" + 0.010*\"handicap\" + 0.009*\"sports\"',\n",
       " 1: '0.020*\"miss\" + 0.020*\"bride\" + 0.011*\"wedding\" + 0.011*\"black\" + 0.010*\"white\" + 0.010*\"ceremony\" + 0.010*\"marriage\" + 0.009*\"friends\" + 0.009*\"happy\" + 0.008*\"pretty\"',\n",
       " 2: '0.032*\"railway\" + 0.020*\"meredith\" + 0.017*\"minister\" + 0.015*\"line\" + 0.013*\"letters\" + 0.010*\"otago\" + 0.010*\"heriot\" + 0.009*\"roxburgh\" + 0.007*\"ashley\" + 0.007*\"league\"',\n",
       " 3: '0.016*\"ashley\" + 0.015*\"vote\" + 0.014*\"electors\" + 0.012*\"prohibition\" + 0.012*\"election\" + 0.011*\"votes\" + 0.010*\"district\" + 0.010*\"license\" + 0.009*\"good\" + 0.008*\"licensing\"',\n",
       " 4: '0.030*\"court\" + 0.017*\"case\" + 0.010*\"evidence\" + 0.009*\"defendant\" + 0.009*\"prisoner\" + 0.008*\"police\" + 0.008*\"law\" + 0.008*\"said\" + 0.007*\"plaintiff\" + 0.007*\"judge\"',\n",
       " 5: '0.007*\"gas\" + 0.006*\"old\" + 0.005*\"years\" + 0.005*\"harle\" + 0.004*\"buddo\" + 0.004*\"billy\" + 0.004*\"he\\'s\" + 0.004*\"new\" + 0.004*\"day\" + 0.003*\"light\"',\n",
       " 6: '0.015*\"fire\" + 0.013*\"ngati\" + 0.012*\"people\" + 0.008*\"tuhoe\" + 0.007*\"tribes\" + 0.007*\"came\" + 0.006*\"accident\" + 0.006*\"occurred\" + 0.006*\"old\" + 0.005*\"went\"',\n",
       " 7: '0.009*\"last\" + 0.009*\"new\" + 0.008*\"port\" + 0.007*\"may\" + 0.006*\"town\" + 0.005*\"arrived\" + 0.005*\"sydney\" + 0.005*\"place\" + 0.004*\"time\" + 0.004*\"vessel\"',\n",
       " 8: '0.016*\"cent\" + 0.016*\"fruit\" + 0.010*\"wheat\" + 0.010*\"grain\" + 0.009*\"esq\" + 0.008*\"prize\" + 0.008*\"plant\" + 0.007*\"bassett\" + 0.007*\"foreign\" + 0.007*\"best\"',\n",
       " 9: '0.069*\"tons\" + 0.056*\"schooner\" + 0.027*\"agents\" + 0.025*\"agent\" + 0.024*\"barque\" + 0.016*\"brig\" + 0.013*\"bay\" + 0.013*\"day\" + 0.012*\"nelson\" + 0.012*\"port\"',\n",
       " 10: '0.015*\"half\" + 0.012*\"water\" + 0.012*\"put\" + 0.009*\"cold\" + 0.009*\"salt\" + 0.009*\"two\" + 0.007*\"bread\" + 0.007*\"three\" + 0.007*\"eggs\" + 0.007*\"sugar\"',\n",
       " 11: '0.041*\"meeting\" + 0.023*\"committee\" + 0.015*\"held\" + 0.012*\"messrs\" + 0.012*\"members\" + 0.009*\"chairman\" + 0.009*\"chair\" + 0.009*\"present\" + 0.009*\"secretary\" + 0.009*\"read\"',\n",
       " 12: '0.014*\"recom\" + 0.014*\"prevalent\" + 0.013*\"taste\" + 0.013*\"stimulant\" + 0.013*\"portable\" + 0.012*\"suits\" + 0.009*\"water\" + 0.007*\"30s\" + 0.007*\"brown\\'s\" + 0.007*\"last\"',\n",
       " 13: '0.123*\"tho\" + 0.026*\"water\" + 0.006*\"two\" + 0.005*\"may\" + 0.005*\"feet\" + 0.005*\"long\" + 0.005*\"air\" + 0.004*\"used\" + 0.004*\"inches\" + 0.004*\"tlio\"',\n",
       " 14: '0.015*\"would\" + 0.008*\"land\" + 0.007*\"made\" + 0.004*\"could\" + 0.004*\"well\" + 0.004*\"much\" + 0.004*\"party\" + 0.004*\"present\" + 0.004*\"last\" + 0.003*\"may\"',\n",
       " 15: '0.009*\"said\" + 0.006*\"could\" + 0.006*\"time\" + 0.006*\"like\" + 0.005*\"old\" + 0.005*\"never\" + 0.005*\"little\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"know\"',\n",
       " 16: '0.070*\"shall\" + 0.032*\"claim\" + 0.029*\"prospecting\" + 0.023*\"warden\" + 0.020*\"granted\" + 0.018*\"ground\" + 0.018*\"area\" + 0.017*\"may\" + 0.017*\"grant\" + 0.015*\"gold\"',\n",
       " 17: '0.007*\"work\" + 0.005*\"home\" + 0.005*\"made\" + 0.004*\"well\" + 0.003*\"esther\" + 0.003*\"good\" + 0.003*\"girls\" + 0.003*\"school\" + 0.003*\"procession\" + 0.003*\"collection\"',\n",
       " 18: '0.007*\"many\" + 0.005*\"new\" + 0.005*\"time\" + 0.005*\"good\" + 0.004*\"last\" + 0.004*\"great\" + 0.004*\"district\" + 0.004*\"years\" + 0.003*\"may\" + 0.003*\"day\"',\n",
       " 19: '0.007*\"great\" + 0.006*\"london\" + 0.005*\"england\" + 0.005*\"queen\" + 0.005*\"years\" + 0.005*\"women\" + 0.004*\"new\" + 0.004*\"royal\" + 0.004*\"two\" + 0.004*\"many\"',\n",
       " 20: '0.020*\"sheep\" + 0.018*\"cattle\" + 0.012*\"milk\" + 0.011*\"cows\" + 0.010*\"fine\" + 0.009*\"mcgrath\" + 0.009*\"cow\" + 0.008*\"rain\" + 0.008*\"socialism\" + 0.008*\"good\"',\n",
       " 21: '0.015*\"lady\" + 0.009*\"daughter\" + 0.008*\"capt\" + 0.007*\"deals\" + 0.007*\"subdivision\" + 0.007*\"formerly\" + 0.007*\"mrs\" + 0.006*\"london\" + 0.006*\"children\" + 0.005*\"rev\"',\n",
       " 22: '0.052*\"board\" + 0.025*\"water\" + 0.017*\"road\" + 0.013*\"race\" + 0.009*\"meeting\" + 0.009*\"bridge\" + 0.009*\"received\" + 0.008*\"creek\" + 0.008*\"supply\" + 0.007*\"messrs\"',\n",
       " 23: '0.054*\"auckland\" + 0.027*\"mrs\" + 0.026*\"sydney\" + 0.022*\"wellington\" + 0.021*\"messrs\" + 0.019*\"england\" + 0.015*\"miss\" + 0.014*\"melbourne\" + 0.012*\"new\" + 0.011*\"misses\"',\n",
       " 24: '0.011*\"play\" + 0.008*\"time\" + 0.007*\"game\" + 0.007*\"library\" + 0.006*\"ball\" + 0.006*\"team\" + 0.006*\"played\" + 0.005*\"ancient\" + 0.005*\"great\" + 0.005*\"good\"',\n",
       " 25: '0.007*\"undermined\" + 0.005*\"number\" + 0.005*\"would\" + 0.004*\"dawson\" + 0.004*\"acetylene\" + 0.003*\"drawn\" + 0.003*\"said\" + 0.003*\"five\" + 0.003*\"acid\" + 0.003*\"first\"',\n",
       " 26: '0.066*\"oxford\" + 0.031*\"eucalyptus\" + 0.022*\"brand\" + 0.021*\"pleasant\" + 0.017*\"local\" + 0.015*\"east\" + 0.013*\"free\" + 0.012*\"option\" + 0.012*\"get\" + 0.012*\"use\"',\n",
       " 27: '0.011*\"cycling\" + 0.008*\"cyclists\" + 0.007*\"machine\" + 0.007*\"road\" + 0.006*\"bicycle\" + 0.005*\"way\" + 0.005*\"cycle\" + 0.005*\"riding\" + 0.005*\"wheel\" + 0.004*\"racing\"',\n",
       " 28: '0.027*\"miss\" + 0.018*\"song\" + 0.010*\"programme\" + 0.010*\"mrs\" + 0.009*\"audience\" + 0.008*\"sang\" + 0.008*\"concert\" + 0.007*\"music\" + 0.006*\"first\" + 0.006*\"songs\"',\n",
       " 29: '0.009*\"tho\" + 0.009*\"rotorua\" + 0.008*\"last\" + 0.006*\"place\" + 0.005*\"auckland\" + 0.004*\"road\" + 0.004*\"district\" + 0.004*\"night\" + 0.004*\"day\" + 0.003*\"friday\"',\n",
       " 30: '0.011*\"bill\" + 0.010*\"new\" + 0.010*\"government\" + 0.009*\"house\" + 0.007*\"sir\" + 0.007*\"bank\" + 0.007*\"lord\" + 0.005*\"zealand\" + 0.005*\"premier\" + 0.005*\"party\"',\n",
       " 31: '0.009*\"would\" + 0.007*\"may\" + 0.006*\"good\" + 0.006*\"people\" + 0.005*\"many\" + 0.004*\"every\" + 0.004*\"must\" + 0.004*\"say\" + 0.004*\"like\" + 0.004*\"time\"',\n",
       " 32: '0.010*\"house\" + 0.009*\"said\" + 0.008*\"would\" + 0.006*\"horse\" + 0.005*\"two\" + 0.005*\"police\" + 0.005*\"made\" + 0.004*\"last\" + 0.004*\"constable\" + 0.004*\"day\"',\n",
       " 33: '0.014*\"government\" + 0.013*\"natives\" + 0.010*\"esq\" + 0.010*\"excellency\" + 0.008*\"settlers\" + 0.008*\"governor\" + 0.007*\"sir\" + 0.007*\"meeting\" + 0.007*\"would\" + 0.007*\"public\"',\n",
       " 34: '0.018*\"would\" + 0.018*\"board\" + 0.011*\"town\" + 0.010*\"stated\" + 0.009*\"matter\" + 0.006*\"chairman\" + 0.006*\"decided\" + 0.005*\"street\" + 0.005*\"meeting\" + 0.005*\"could\"',\n",
       " 35: '0.011*\"work\" + 0.011*\"stone\" + 0.010*\"last\" + 0.009*\"week\" + 0.008*\"gold\" + 0.008*\"lyell\" + 0.007*\"company\" + 0.007*\"feet\" + 0.007*\"good\" + 0.006*\"dredge\"',\n",
       " 36: '0.027*\"death\" + 0.019*\"deceased\" + 0.014*\"child\" + 0.011*\"body\" + 0.010*\"years\" + 0.010*\"died\" + 0.010*\"jury\" + 0.009*\"mrs\" + 0.008*\"came\" + 0.008*\"father\"',\n",
       " 37: '0.044*\"cases\" + 0.036*\"casks\" + 0.035*\"case\" + 0.028*\"bags\" + 0.022*\"boxes\" + 0.020*\"flour\" + 0.018*\"tobacco\" + 0.018*\"cask\" + 0.017*\"barrels\" + 0.016*\"oil\"',\n",
       " 38: '0.012*\"miles\" + 0.012*\"land\" + 0.012*\"river\" + 0.007*\"water\" + 0.007*\"two\" + 0.006*\"acres\" + 0.006*\"north\" + 0.006*\"road\" + 0.005*\"feet\" + 0.005*\"side\"',\n",
       " 39: '0.006*\"fire\" + 0.006*\"house\" + 0.005*\"lane\" + 0.005*\"well\" + 0.005*\"hall\" + 0.005*\"new\" + 0.004*\"tea\" + 0.004*\"building\" + 0.004*\"watches\" + 0.004*\"room\"',\n",
       " 40: '0.021*\"oxford\" + 0.019*\"school\" + 0.013*\"evening\" + 0.012*\"church\" + 0.011*\"held\" + 0.009*\"last\" + 0.008*\"good\" + 0.008*\"children\" + 0.008*\"rev\" + 0.008*\"meeting\"',\n",
       " 41: '0.033*\"shall\" + 0.021*\"paper\" + 0.019*\"papers\" + 0.017*\"council\" + 0.015*\"postage\" + 0.014*\"subscribers\" + 0.014*\"upon\" + 0.013*\"borough\" + 0.013*\"office\" + 0.012*\"must\"',\n",
       " 42: '0.027*\"parish\\'s\" + 0.012*\"women\" + 0.010*\"beat\" + 0.008*\"ryde\" + 0.005*\"weld\" + 0.005*\"john\" + 0.004*\"landlords\" + 0.004*\"without\" + 0.003*\"many\" + 0.003*\"turkey\"',\n",
       " 43: '0.024*\"would\" + 0.012*\"government\" + 0.010*\"land\" + 0.006*\"money\" + 0.006*\"colony\" + 0.006*\"present\" + 0.006*\"could\" + 0.005*\"people\" + 0.005*\"act\" + 0.005*\"public\"',\n",
       " 44: '0.008*\"life\" + 0.007*\"seconds\" + 0.007*\"greece\" + 0.005*\"crete\" + 0.005*\"much\" + 0.004*\"would\" + 0.004*\"wearing\" + 0.004*\"tne\" + 0.004*\"mars\" + 0.003*\"able\"',\n",
       " 45: '0.013*\"thy\" + 0.010*\"thou\" + 0.008*\"possible\" + 0.007*\"paget\" + 0.006*\"blank\" + 0.005*\"ingram\" + 0.005*\"thee\" + 0.004*\"addington\" + 0.004*\"world\" + 0.004*\"ancall\"',\n",
       " 46: '0.046*\"000\" + 0.013*\"year\" + 0.010*\"years\" + 0.007*\"cent\" + 0.007*\"new\" + 0.006*\"market\" + 0.006*\"100\" + 0.005*\"value\" + 0.005*\"trade\" + 0.005*\"last\"',\n",
       " 47: '0.014*\"new\" + 0.010*\"zealand\" + 0.010*\"would\" + 0.008*\"land\" + 0.008*\"may\" + 0.008*\"upon\" + 0.006*\"colony\" + 0.005*\"company\" + 0.004*\"made\" + 0.004*\"port\"',\n",
       " 48: '0.081*\"gold\" + 0.048*\"standard\" + 0.027*\"silver\" + 0.010*\"two\" + 0.010*\"paper\" + 0.009*\"row\" + 0.008*\"knit\" + 0.008*\"metal\" + 0.006*\"hogs\" + 0.006*\"pork\"',\n",
       " 49: '0.009*\"captain\" + 0.009*\"two\" + 0.007*\"ship\" + 0.007*\"killed\" + 0.007*\"vessel\" + 0.006*\"troops\" + 0.006*\"boat\" + 0.006*\"british\" + 0.006*\"said\" + 0.005*\"officers\"'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output shows that 33, 40 are of interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
