{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joshua Black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring nb2 subset with text analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import logging\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore, TfidfModel\n",
    "from gensim.matutils import corpus2csc\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_cytoscape as cyto\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "import NL_helpers\n",
    "import NL_topicmodels # Will need to generate BOW using function in topic models.\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "TOKENIZER = RegexpTokenizer(r\"[A-Za-z']+\")\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "WORDS = set(words.words()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to reload NL_helpers and NL_topicmodels if they have been changed.\n",
    "from importlib import reload\n",
    "reload(NL_helpers)\n",
    "reload(NL_topicmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already generated a dataset containing all articles with the regex search term 'philoso*' run on the NL library. I load it as follows:\n",
    "\n",
    "**Note, error:** the step at which I apply the search is before the text has been converted to lower case. The results of the re search are case sensitive. I had intended to include, e.g. 'Philosophy' as well. I don't expect that this will cause too many problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df = pd.read_pickle('pickles/nb2_philoso_df_v2.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df['Tokenised'] = philoso_df['Text'].apply(\n",
    "    lambda x: TOKENIZER.tokenize(NL_helpers.blocks2string(x).lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates ought to be removed from 'subset' corpus, but have already been removed from full 'philoso*' corpus.\n",
    "# philoso_df = philoso_df[~philoso_df.astype(str).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ODT        2867\n",
       "ESD        2733\n",
       "OW         2699\n",
       "AS         2231\n",
       "LT         2173\n",
       "CHP        1881\n",
       "BH         1594\n",
       "NZTIM      1391\n",
       "TC          986\n",
       "NEM         849\n",
       "HBH         831\n",
       "DSC         763\n",
       "OAM         710\n",
       "NOT         661\n",
       "MEX         583\n",
       "WH          570\n",
       "WSTAR       549\n",
       "WC          545\n",
       "DTN         527\n",
       "WT          521\n",
       "LWM         512\n",
       "CROMARG     455\n",
       "WCT         436\n",
       "ME          386\n",
       "GRA         385\n",
       "AG          361\n",
       "WI          289\n",
       "WDT         248\n",
       "MIC         230\n",
       "FS          209\n",
       "DUNST       162\n",
       "IT          155\n",
       "OO          149\n",
       "WAIST       139\n",
       "WEST        138\n",
       "LCP         135\n",
       "BA          124\n",
       "HBT         123\n",
       "CL          112\n",
       "HNS         102\n",
       "MH           96\n",
       "MS           92\n",
       "MT           75\n",
       "WOODEX       74\n",
       "NA           58\n",
       "KUMAT        52\n",
       "HAST         49\n",
       "OG           32\n",
       "NZSCSG       26\n",
       "OPUNT        15\n",
       "HLC          14\n",
       "NZCPNA        8\n",
       "HBWT          8\n",
       "NZGWS         7\n",
       "WDA           6\n",
       "MTBM          2\n",
       "ALG           2\n",
       "CHARG         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts by newspaper\n",
    "philoso_df.index.map(lambda x: x[0:x.find('_')]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f965cb413d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counts by year.\n",
    "philoso_df.index.map(lambda x: int(x[x.find('_')+1:x.find('_')+5])).to_series().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting a subset of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a random subset of the corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = sample(list(philoso_df.index), 100)\n",
    "interact(NL_helpers.html_text, index=sample_indices, dataframe=fixed(philoso_df), boldface=fixed('philoso*\\\\w*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes: * (Note your random sample will be different)\n",
    " - AS_18860821_ARTICLE40: Example of argument with 'freethinker' over grouding ethics(?) in 'natural right' vs Christian approach. Interesting as lower limit in terms of OCR quality?\n",
    " - Use of 'philosophical' to mean deep, resigned, detatched.\n",
    " - 'philosophers and sceptics' vs Christians.\n",
    " - CHP_18980425_ARTICLE60: Report of what is issued at the wellington public library in 1898 - 'philosophical subjects are almost entire;y neglected'\n",
    " - Discussion of Darwinism as new philosophy.\n",
    " - Discussions of e.g. Stoicism.\n",
    " - 1893 Hosking on Miricles, listed as 'popular lecture'. CHP_18930515_ARTICLE22\n",
    " - **In general, quite a few reprints of exerpts from texts. (e.g. BH_18890222_ARTICLE33)\n",
    " - Robert Stout comes up a lot as an early political figure who is thought of as having, e.g. 'a philosophic mind'.\n",
    " - Some lecture reports coming through (e.g. AS_18800817_ARTICLE22, CHP_18690823_ARTICLE15)\n",
    " - Theosophy coming through\n",
    " - Quite a lot of extracts of fiction (one with a Hegelian character).\n",
    " - CL_18790919_ARTICLE27 - first-order philosophy - including definition of 'philosophy' in Clutha Leader.\n",
    " - CHP_18920926_ARTICLE10 - good letters to editor on 'freethought' and christianity\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by quickly looking through concordances for key words. The initial 'flat list' of tokens is taken from a random sample of 1000 items in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for tokens in philoso_df['Tokenised'].sample(n=1000, random_state=1):\n",
    "    for token in tokens:\n",
    "        flat_list.append(token)\n",
    "sample_text = Text(flat_list)\n",
    "del flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text.concordance('philosophy', width=100, lines=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_text.concordance('lecture', width=100, lines=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations shows which words have a tendency to appear together *near one another* in documents (cf. co-occurrence). The ranking of word pairs depends on the statistic chosen.\n",
    "\n",
    "**PMI:** pointwise mutual information is a measure from information theory. Suppose we have probability distributions for each word, *x* and *y*, we might wonder how much information about p(x) is carried by p(y). If the words *x* and *y* always occur together, then *pmi* will be very high. This makes PMI a good measure of word occurence (dreadful - come back once you've reminded yourself how this works).\n",
    "\n",
    "**Likelihood ratio:**\n",
    "\n",
    "The code below collects all immediate bigrams for a window of size 2 and of size 5. We then filter our bigrams which do not contain 'philosophy', those which appear less than three times, and those which contain stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again build up a flat list, but this time with the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for tokens in philoso_df['Tokenised']:\n",
    "    for token in tokens:\n",
    "        flat_list.append(token)\n",
    "all_text = Text(flat_list)\n",
    "del flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_filter = lambda *w: 'philosophy' not in w\n",
    "stopword_filter = lambda w: w in STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf = BigramCollocationFinder.from_words(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf.apply_ngram_filter(philoso_filter)\n",
    "bcf.apply_word_filter(stopword_filter)\n",
    "bcf.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_ws2 = bcf.nbest(bm.pmi, 50)\n",
    "pmi_ws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ws2 = bcf.nbest(bm.likelihood_ratio, 50)\n",
    "lr_ws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_not_lr = []\n",
    "for collocation in pmi_ws2:\n",
    "    if collocation not in lr_ws2:\n",
    "        pmi_not_lr.append(collocation)\n",
    "pmi_not_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_not_pmi = []\n",
    "for collocation in lr_ws2:\n",
    "    if collocation not in pmi_ws2:\n",
    "        lr_not_pmi.append(collocation)\n",
    "lr_not_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences here don't seem to make a big difference to the conclusions above. Not surprising as they are very closely related measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bcf\n",
    "del all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also expand the window to consider words either five spaces to the left or right of philosophy. (Window size includes stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_big = BigramCollocationFinder.from_words(all_text, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_big.apply_ngram_filter(philoso_filter)\n",
    "bcf_big.apply_word_filter(stopword_filter)\n",
    "bcf_big.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_ws5 = bcf_big.nbest(bm.pmi, 50)\n",
    "pmi_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_ws5 = bcf_big.nbest(bm.likelihood_ratio, 50)\n",
    "lr_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_not_lr_ws5 = []\n",
    "for collocation in pmi_ws5:\n",
    "    if collocation not in lr_ws5:\n",
    "        pmi_not_lr_ws5.append(collocation)\n",
    "pmi_not_lr_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_not_pmi_ws5 = []\n",
    "for collocation in lr_ws5:\n",
    "    if collocation not in pmi_ws5:\n",
    "        lr_not_pmi_ws5.append(collocation)\n",
    "lr_not_pmi_ws5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr_not_pmi_ws5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bcf_big, all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences in scoring more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform document-level cooccurence analysis we will shift from NLTK to gensim. We first build a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary is much too large for our purposes (at ~70k words). We can filter stop words and all one and two letter words from the tokenized text to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "philoso_df['Tokenised'] = philoso_df['Tokenised'].map(lambda x: [word for word in x if len(word)>2 and not word in STOPWORDS and word in WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-26 23:46:16,069 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-26 23:46:18,641 : INFO : adding document #10000 to Dictionary(29796 unique tokens: ['appear', 'ashamed', 'atmos', 'beautiful', 'become']...)\n",
      "2021-01-26 23:46:21,330 : INFO : adding document #20000 to Dictionary(35394 unique tokens: ['appear', 'ashamed', 'atmos', 'beautiful', 'become']...)\n",
      "2021-01-26 23:46:24,102 : INFO : adding document #30000 to Dictionary(38834 unique tokens: ['appear', 'ashamed', 'atmos', 'beautiful', 'become']...)\n",
      "2021-01-26 23:46:24,488 : INFO : built Dictionary(39202 unique tokens: ['appear', 'ashamed', 'atmos', 'beautiful', 'become']...) from 31131 documents (total 7656141 corpus positions)\n",
      "2021-01-26 23:46:24,570 : INFO : discarding 29725 tokens: [('appear', 3143), ('become', 5860), ('best', 6763), ('better', 6852), ('character', 6357), ('even', 12177), ('every', 13016), ('existence', 4658), ('far', 9655), ('feel', 3664)]...\n",
      "2021-01-26 23:46:24,571 : INFO : keeping 9477 tokens which were in no less than 50 and no more than 3113 (=10.0%) documents\n",
      "2021-01-26 23:46:24,586 : INFO : resulting dictionary: Dictionary(9477 unique tokens: ['ashamed', 'atmos', 'beautiful', 'blush', 'bub']...)\n"
     ]
    }
   ],
   "source": [
    "minimum_in_docs = 50 # 10\n",
    "max_prop = 0.1\n",
    "dictionary = corpora.Dictionary(philoso_df['Tokenised'])\n",
    "dictionary.filter_extremes(no_below=minimum_in_docs, no_above=max_prop)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-19 10:03:28,096 : INFO : saving Dictionary object under dictionaries/nb2_philoso_df_min50_max10perc.dict, separately None\n",
      "2021-01-19 10:03:28,555 : INFO : saved dictionaries/nb2_philoso_df_min50_max10perc.dict\n"
     ]
    }
   ],
   "source": [
    "dictionary.save(f'dictionaries/nb2_philoso_df_min{minimum_in_docs}_max{int(max_prop*100)}perc.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus = NL_topicmodels.NL_corpus(philoso_df, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-26 23:46:38,279 : WARNING : constructor received both corpus and explicit inverse document frequencies; ignoring the corpus\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = TfidfModel(philo_corpus, dictionary=philo_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items['TF-IDF'] = tfidf_model[philo_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_corpus.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes over time? - I don't think this is helping me. Need some more non-philosophy as a background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = corpus2csc(philo_corpus.items['BOW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame.sparse.from_spmatrix(sparse)\n",
    "del sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.index = philo_corpus.dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31121</th>\n",
       "      <th>31122</th>\n",
       "      <th>31123</th>\n",
       "      <th>31124</th>\n",
       "      <th>31125</th>\n",
       "      <th>31126</th>\n",
       "      <th>31127</th>\n",
       "      <th>31128</th>\n",
       "      <th>31129</th>\n",
       "      <th>31130</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ashamed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmos</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blush</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bub</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognize</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>korero</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roto</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tana</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9477 rows × 31131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6      7      8      \\\n",
       "ashamed      1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "atmos        1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "beautiful    2.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "blush        1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "bub          1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...          ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "recognize    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "toa          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "korero       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "roto         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "tana         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "           9      ...  31121  31122  31123  31124  31125  31126  31127  31128  \\\n",
       "ashamed      0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "atmos        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "beautiful    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "blush        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "bub          0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...          ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "recognize    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "toa          0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "korero       0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "roto         0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "tana         0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "           31129  31130  \n",
       "ashamed      0.0    0.0  \n",
       "atmos        0.0    0.0  \n",
       "beautiful    0.0    0.0  \n",
       "blush        0.0    0.0  \n",
       "bub          0.0    0.0  \n",
       "...          ...    ...  \n",
       "recognize    0.0    0.0  \n",
       "toa          0.0    0.0  \n",
       "korero       0.0    0.0  \n",
       "roto         0.0    0.0  \n",
       "tana         0.0    0.0  \n",
       "\n",
       "[9477 rows x 31131 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.to_pickle('pickles/dtm_nb2_v2_BOW_9kwords.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df = dtm.dot(dtm.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df.to_pickle('pickles/ttm_nb2_v2_BOW_9kwords.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ashamed</th>\n",
       "      <th>atmos</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blush</th>\n",
       "      <th>bub</th>\n",
       "      <th>build</th>\n",
       "      <th>callous</th>\n",
       "      <th>capable</th>\n",
       "      <th>close</th>\n",
       "      <th>confined</th>\n",
       "      <th>...</th>\n",
       "      <th>declamation</th>\n",
       "      <th>unsolved</th>\n",
       "      <th>advertiser</th>\n",
       "      <th>coo</th>\n",
       "      <th>intentional</th>\n",
       "      <th>recognize</th>\n",
       "      <th>toa</th>\n",
       "      <th>korero</th>\n",
       "      <th>roto</th>\n",
       "      <th>tana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ashamed</th>\n",
       "      <td>890.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmos</th>\n",
       "      <td>2.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>83.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5881.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blush</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bub</th>\n",
       "      <td>40.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4344.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognize</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>korero</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5748.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>1652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roto</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>896.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tana</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>896.0</td>\n",
       "      <td>1299.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9477 rows × 9477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ashamed  atmos  beautiful  blush     bub  build  callous  capable  \\\n",
       "ashamed      890.0    2.0       83.0   20.0    40.0   38.0      7.0     62.0   \n",
       "atmos          2.0  143.0       27.0    3.0     6.0    7.0      2.0     28.0   \n",
       "beautiful     83.0   27.0     5881.0   81.0   428.0  267.0     38.0    449.0   \n",
       "blush         20.0    3.0       81.0  503.0    16.0   25.0      4.0     36.0   \n",
       "bub           40.0    6.0      428.0   16.0  4344.0   90.0      7.0    152.0   \n",
       "...            ...    ...        ...    ...     ...    ...      ...      ...   \n",
       "recognize      5.0    1.0       23.0    0.0     0.0    5.0      0.0     16.0   \n",
       "toa            0.0    0.0        9.0    2.0     6.0    3.0      0.0      4.0   \n",
       "korero         0.0    0.0        0.0    0.0     0.0    0.0      0.0      0.0   \n",
       "roto           0.0    0.0        0.0    0.0     0.0    1.0      0.0      0.0   \n",
       "tana           0.0    0.0        2.0    0.0     0.0    1.0      0.0      0.0   \n",
       "\n",
       "           close  confined  ...  declamation  unsolved  advertiser  coo  \\\n",
       "ashamed     57.0      26.0  ...          3.0       1.0         2.0  1.0   \n",
       "atmos       25.0       3.0  ...          0.0       1.0         0.0  0.0   \n",
       "beautiful  464.0     175.0  ...         14.0      14.0        11.0  6.0   \n",
       "blush       29.0      22.0  ...          0.0       1.0         0.0  0.0   \n",
       "bub        181.0      76.0  ...          7.0       2.0         2.0  7.0   \n",
       "...          ...       ...  ...          ...       ...         ...  ...   \n",
       "recognize   14.0       5.0  ...          2.0       1.0         0.0  0.0   \n",
       "toa          9.0       3.0  ...          0.0       0.0         0.0  1.0   \n",
       "korero       0.0       0.0  ...          0.0       0.0         0.0  0.0   \n",
       "roto         0.0       0.0  ...          0.0       1.0         0.0  0.0   \n",
       "tana         0.0       0.0  ...          1.0       0.0         0.0  0.0   \n",
       "\n",
       "           intentional  recognize   toa  korero    roto    tana  \n",
       "ashamed            3.0        5.0   0.0     0.0     0.0     0.0  \n",
       "atmos              1.0        1.0   0.0     0.0     0.0     0.0  \n",
       "beautiful         18.0       23.0   9.0     0.0     0.0     2.0  \n",
       "blush              0.0        0.0   2.0     0.0     0.0     0.0  \n",
       "bub                2.0        0.0   6.0     0.0     0.0     0.0  \n",
       "...                ...        ...   ...     ...     ...     ...  \n",
       "recognize          0.0       71.0   0.0     0.0     0.0     2.0  \n",
       "toa                0.0        0.0  74.0    93.0    86.0    78.0  \n",
       "korero             0.0        0.0  93.0  5748.0  1916.0  1652.0  \n",
       "roto               0.0        0.0  86.0  1916.0  1164.0   896.0  \n",
       "tana               0.0        2.0  78.0  1652.0   896.0  1299.0  \n",
       "\n",
       "[9477 rows x 9477 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html\n",
    "def dice_coocs(term, dtm, ttm, num_coocs):\n",
    "    \"\"\"Return num_coocs with dice statistics given search term\n",
    "    document-term matrix and term-term matrix. Return as \n",
    "    pandas series with terms as indices and significances as values..\n",
    "    ttm and dtm are pandas dataframes.\"\"\"\n",
    "    #num_documents = len(dtm.columns)\n",
    "    all_term_occurrences = dtm.sum(axis=1)\n",
    "    term_occurrences = all_term_occurrences[term]\n",
    "    cooccurrences = ttm.loc[term]\n",
    "    dicesig = 2 * cooccurrences / (term_occurrences + all_term_occurrences)\n",
    "    dicesig = dicesig.sort_values(ascending=False)[0:num_coocs]\n",
    "    return dicesig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicesig = 2 * cooccurrences / (term_occurrences + all_term_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dicesig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicesig.sort_values(ascending=False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coocs('philosophy', binary_dtm, tt_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_helpers.log_dice_coocs('infinite', dtm, tt_df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_cooc = dice_coocs('philosophy', binary_dtm, tt_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_network = NL_helpers.network_dataframe(\n",
    "    term='philosophy', \n",
    "    stat='ml', \n",
    "    dtm=dtm,\n",
    "    ttm=tt_df,\n",
    "    num_coocs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_network_df = pd.DataFrame(data=philo_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "for item in philo_cooc.iteritems():\n",
    "    from_list = network.get('source', [])\n",
    "    from_list.append('philosophy')\n",
    "    network['source'] = from_list\n",
    "    to_list = network.get('target', [])\n",
    "    to_list.append(item[0])\n",
    "    network['target'] = to_list\n",
    "    weight_list = network.get('weight', [])\n",
    "    weight_list.append(item[1])\n",
    "    network['weight'] = weight_list\n",
    "    \n",
    "    item_coocs = dice_coocs(item[0], binary_dtm, tt_df, 10)\n",
    "    for sub_item in item_coocs.iteritems():\n",
    "        if item[0] != sub_item[0]:\n",
    "            from_list = network.get('source', [])\n",
    "            from_list.append(item[0])\n",
    "            network['source'] = from_list\n",
    "            to_list = network.get('target', [])\n",
    "            to_list.append(sub_item[0])\n",
    "            network['target'] = to_list\n",
    "            weight_list = network.get('weight', [])\n",
    "            weight_list.append(sub_item[1])\n",
    "            network['weight'] = weight_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.DataFrame(data=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.convert_matrix.from_pandas_edgelist(philo_network_df, edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = G.edges()\n",
    "weights = [np.exp(G[u][v]['weight']) * 0.01 for u,v in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nx.degree(G)\n",
    "sizes = [(d[node]+1) * 100 for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = nx.drawing.layout.spring_layout(G, k=1/np.sqrt(len(G.nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "# Visualize graph components\n",
    "nx.draw_networkx_edges(G, layout, alpha=0.5, width=weights, edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G, layout, node_size=sizes, node_color=\"#210070\", alpha=0.9)\n",
    "label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}\n",
    "labels = nx.draw_networkx_labels(G, layout, font_size=11, bbox=label_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_net = NL_helpers.network_dash(\n",
    "    term='philosophy', \n",
    "    stat='log dice', \n",
    "    dtm=dtm,\n",
    "    ttm=tt_df,\n",
    "    num_coocs=25,\n",
    "    sec_coocs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)\n",
    "\n",
    "philo_cytoscape = cyto.Cytoscape(\n",
    "        id='philosophy-network',\n",
    "        minZoom=1,\n",
    "        layout={'name': 'cose'},\n",
    "        style={'width': '100%', 'height': '800px'},\n",
    "        elements=philo_net,\n",
    "        stylesheet=[\n",
    "            {\n",
    "                'selector': 'edge',\n",
    "                'style': {\n",
    "                    'width': 'mapData(weight, 3, 6, 1, 3)',\n",
    "                    'line-color': 'silver'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'selector': 'node',\n",
    "                'style': {\n",
    "                    'content': 'data(label)',\n",
    "                    'width': 'mapData(size, 1, 10, 10, 20)',\n",
    "                    'height': 'mapData(size, 1, 10, 10, 20)'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'selector': 'label',\n",
    "                'style': {\n",
    "                    'font-size': 6,\n",
    "                    'text-valign': 'center',\n",
    "                    'text-background-color': 'white',\n",
    "                    'text-background-opacity': 0.6,\n",
    "                    'text-background-padding': 1,\n",
    "                    'text-border-color': 'black',\n",
    "                    'text-border-opacity': 1,\n",
    "                    'text-border-width': 0.5\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H2(\"Cooccurence Networks (BOW)\"),\n",
    "    html.P(\"Search Term:\"),\n",
    "    dcc.Input(\n",
    "        id='search-term',\n",
    "        type='text',\n",
    "        value='philosophy'\n",
    "    ),\n",
    "    html.P(\"Statistic:\"),\n",
    "    dcc.Dropdown(\n",
    "        id='stat-choice',\n",
    "        options=[\n",
    "            {'label': 'Mutual likelihood', 'value': 'ml'},\n",
    "            {'label': 'Log Dice', 'value': 'log dice'}\n",
    "        ],\n",
    "        value='ml'\n",
    "    ),\n",
    "    html.Button('Submit', id='submit-val', n_clicks=0),\n",
    "    philo_cytoscape\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='philosophy-network', component_property='elements'),\n",
    "    Input(component_id='submit-val', component_property='n_clicks'),\n",
    "    State(component_id='stat-choice', component_property='value'),\n",
    "    State(component_id='search-term', component_property='value'),\n",
    ")\n",
    "def update_network_stat(n_clicks, stat_value, search_value):\n",
    "    network = NL_helpers.network_dash(\n",
    "        term=search_value, \n",
    "        stat=stat_value, \n",
    "        dtm=dtm,\n",
    "        ttm=tt_df,\n",
    "        num_coocs=10,\n",
    "        sec_coocs=5\n",
    "    )\n",
    "    return network \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some candidate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_helpers.interactive_text_search(philoso_df, \"[Mm]ill\\'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_helpers.interactive_text_search(philoso_df, '[Hh]artley')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
